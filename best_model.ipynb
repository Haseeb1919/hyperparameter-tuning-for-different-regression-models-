{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regression Models with Hyperparameter Tuning and Pipeline**\n",
    "- **Context**:\n",
    "  This Jupyter notebook explores regression models for predictive analysis, incorporating hyperparameter tuning and pipelines. By optimizing hyperparameters, we enhance model performance and generalization. Using pipelines and preprocessing techniques streamlines the modeling process.\n",
    "   \n",
    "- **Objective**:\n",
    "  This notebook demonstrates hyperparameter tuning for regression models, integrating pipelines and preprocessing. It aims to find optimal configurations for each model, improving accuracy and predictive power.\n",
    "\n",
    "  This notebook consist of the following sections:\n",
    "    1. **Importing libraries**\n",
    "    2. **Predicting vest model on the base of evaluation metrics**\n",
    "    3. **Hyperparameter tuning for different regression models**\n",
    "    4. **Adding preprocessor to the pipeline**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**NOTE: Tips dataset is used in this notebook for demonstration purposes. The dataset is available in the scikit-learn library. the dataset is not that big, and it is used for demonstration purposes only. Beacuse the main focus of this notebook is to demonstrate the process of hyperparameter tuning for different regression models while integrating pipelines and how to implement preprocessing in pipleline.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import required regressor libraries\n",
    "from sklearn.linear_model import  LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "#import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y',\n",
       "       'z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "df=sns.load_dataset('diamonds')\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select feature and variable\n",
    "x=df.drop('price',axis=1)\n",
    "y=df['price']\n",
    "\n",
    "\n",
    "#label encoding(changing categorical data to numerical data)\n",
    "le=LabelEncoder()\n",
    "x['cut']=le.fit_transform(x['cut'])\n",
    "x['color']=le.fit_transform(x['color'])\n",
    "x['clarity']=le.fit_transform(x['clarity'])\n",
    "\n",
    "\n",
    "#split data\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the best regression model on the base of evaluations metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression mean squared error: 1825912.991525348\n",
      "Linear Regression mean absolute error: 858.7084697710088\n",
      "Linear Regression r2 score: 0.8851397433679632\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m metric_values \u001b[39m=\u001b[39m {}\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m name , model \u001b[39min\u001b[39;00m models\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     16\u001b[0m     \u001b[39m#fit each model from models on training model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     model\u001b[39m.\u001b[39;49mfit(x_train,y_train)\n\u001b[0;32m     19\u001b[0m     \u001b[39m#make prediction on each model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     y_pred\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mpredict(x_test)\n",
      "File \u001b[1;32mc:\\Users\\Haseeb\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Haseeb\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[LibSVM]\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[39m=\u001b[39m rnd\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[0;32m    251\u001b[0m \u001b[39m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\Haseeb\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\sklearn\\svm\\_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    315\u001b[0m libsvm\u001b[39m.\u001b[39mset_verbosity_wrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    317\u001b[0m \u001b[39m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    319\u001b[0m (\n\u001b[0;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_,\n\u001b[0;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_vectors_,\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_support,\n\u001b[0;32m    323\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdual_coef_,\n\u001b[0;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_,\n\u001b[0;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probA,\n\u001b[0;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probB,\n\u001b[0;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_status_,\n\u001b[0;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_iter,\n\u001b[1;32m--> 329\u001b[0m ) \u001b[39m=\u001b[39m libsvm\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    330\u001b[0m     X,\n\u001b[0;32m    331\u001b[0m     y,\n\u001b[0;32m    332\u001b[0m     svm_type\u001b[39m=\u001b[39;49msolver_type,\n\u001b[0;32m    333\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    334\u001b[0m     \u001b[39m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    335\u001b[0m     class_weight\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_class_weight\u001b[39;49m\u001b[39m\"\u001b[39;49m, np\u001b[39m.\u001b[39;49mempty(\u001b[39m0\u001b[39;49m)),\n\u001b[0;32m    336\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[0;32m    337\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[0;32m    338\u001b[0m     nu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu,\n\u001b[0;32m    339\u001b[0m     probability\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability,\n\u001b[0;32m    340\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    341\u001b[0m     shrinking\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinking,\n\u001b[0;32m    342\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m    343\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[0;32m    344\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    345\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[0;32m    346\u001b[0m     epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[0;32m    347\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    348\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m    349\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_from_fit_status()\n",
      "File \u001b[1;32msklearn\\svm\\_libsvm.pyx:265\u001b[0m, in \u001b[0;36msklearn.svm._libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creating a dictionary of models to valuate\n",
    "models = {\n",
    "        'Linear Regression' : LinearRegression(),\n",
    "        'SVR' : SVR(),\n",
    "        'Decision Tree' : DecisionTreeRegressor(),\n",
    "        'Random Forest' : RandomForestRegressor(),\n",
    "        'KNN' : KNeighborsRegressor(),\n",
    "        'AdaBoost' : AdaBoostRegressor(),\n",
    "        'Gradient Boosting' : GradientBoostingRegressor()\n",
    "        }\n",
    "\n",
    "        \n",
    "#train and predict each model with evaluation metric in a for loop and selecting the best model on the basis of each metric\n",
    "metric_values = {}\n",
    "for name , model in models.items():\n",
    "    #fit each model from models on training model\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "    #make prediction on each model\n",
    "    y_pred=model.predict(x_test)\n",
    "    \n",
    "      # Evaluate each model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Store metric values for each model\n",
    "    metric_values[name] = {'mse': mse, 'mae': mae, 'r2_score': r2}\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(name, \"mean squared error:\", mse)\n",
    "    print(name, \"mean absolute error:\", mae)\n",
    "    print(name, \"r2 score:\", r2)\n",
    "    print()\n",
    "\n",
    "# Find the best model based on each metric\n",
    "best_model_mse = min(metric_values, key=lambda x: metric_values[x]['mse'])\n",
    "best_model_mae = min(metric_values, key=lambda x: metric_values[x]['mae'])\n",
    "best_model_r2 = max(metric_values, key=lambda x: metric_values[x]['r2_score'])\n",
    "\n",
    "print(\"Best Model based on MSE:\", best_model_mse)\n",
    "print(\"Best Model based on MAE:\", best_model_mae)\n",
    "print(\"Best Model based on R2 Score:\", best_model_r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning and pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary of models to valuate and doing hyperparameter tuning\n",
    "models = {\n",
    "        'Linear Regression' : (LinearRegression(),{}),\n",
    "\n",
    "        'SVR' : (SVR(),{'kernel':['rbf','poly','sigmoid'], 'C':[0.1,1,10],'gamma':[1,0.1,0.01], 'epsilon':[0.1,0.01,0.001]}),\n",
    "\n",
    "        'Decision Tree' :(DecisionTreeRegressor(),{'splitter':['best','random'],'max_depth':['None',5,10]}),\n",
    "\n",
    "        'Random Forest' : (RandomForestRegressor(),{'n_estimators':[10,50,100],'max_depth':['None',5,10]}),\n",
    "\n",
    "        'KNN' : (KNeighborsRegressor(),{'n_neighbors':np.arange(3,100, 3),'weights':['uniform','distance'],'algorithm':['auto','ball_tree','kd_tree','brute']}), \n",
    "\n",
    "        'AdaBoost' : (AdaBoostRegressor(),{'n_neighbors': np.arange(3,100, 3),'loss':['linear','square','exponential'],'learning_rate':[0.1,0.01,0.001],'n_estimators':[10,100,200]}),\n",
    "        \n",
    "        'Gradient Boosting' : (GradientBoostingRegressor(),{'n_neighbors': np.arange(3,100, 3),'loss':['ls','lad','huber','quantile'],'learning_rate':[0.1,0.01,0.001],'n_estimators':[10,100,200]})\n",
    "        }\n",
    "\n",
    "        \n",
    "#train and predict each model with evaluation metric as well making a for loop\n",
    "metric_values = {}\n",
    "for name , (model, params) in models.items():\n",
    "    #create a pipeline\n",
    "    pipeline=GridSearchCV(model,params,cv=5)\n",
    "\n",
    "    #fit pipeline\n",
    "    pipeline.fit(x_train,y_train)\n",
    "\n",
    "   \n",
    "    #make prediction on each model\n",
    "    y_pred=pipeline.predict(x_test)\n",
    "    \n",
    "      # Evaluate each model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Store metric values for each model\n",
    "    metric_values[name] = {'mse': mse, 'mae': mae, 'r2_score': r2}\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(name, \"mean squared error:\", mse)\n",
    "    print(name, \"mean absolute error:\", mae)\n",
    "    print(name, \"r2 score:\", r2)\n",
    "    print()\n",
    "\n",
    "# Find the best model based on each metric\n",
    "best_model_mse = min(metric_values, key=lambda x: metric_values[x]['mse'])\n",
    "best_model_mae = min(metric_values, key=lambda x: metric_values[x]['mae'])\n",
    "best_model_r2 = max(metric_values, key=lambda x: metric_values[x]['r2_score'])\n",
    "\n",
    "print(\"Best Model based on MSE:\", best_model_mse)\n",
    "print(\"Best Model based on MAE:\", best_model_mae)\n",
    "print(\"Best Model based on R2 Score:\", best_model_r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add preprocessor inside the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a preprocessor\n",
    "preprocessor=ColumnTransformer(\n",
    "    transformers=['numeric_scaling', StandardScaler(),['total_bill', 'size']],remainder='passthrough')\n",
    "\n",
    "\n",
    "#creating a dictionary of models to valuate\n",
    "models = {\n",
    "        'Linear Regression' : (LinearRegression(),{}),\n",
    "\n",
    "        'SVR' : (SVR(),{'kernel':['rbf','poly','sigmoid'], 'C':[0.1,1,10],'gamma':[1,0.1,0.01], 'epsilon':[0.1,0.01,0.001]}),\n",
    "\n",
    "        'Decision Tree' :(DecisionTreeRegressor(),{'splitter':['best','random'],'max_depth':['None',5,10]}),\n",
    "\n",
    "        'Random Forest' : (RandomForestRegressor(),{'n_estimators':[10,50,100],'max_depth':['None',5,10]}),\n",
    "\n",
    "        'KNN' : (KNeighborsRegressor(),{'n_neighbors':np.arange(3,100, 3),'weights':['uniform','distance'],'algorithm':['auto','ball_tree','kd_tree','brute']}), \n",
    "\n",
    "        'AdaBoost' : (AdaBoostRegressor(),{'n_neighbors': np.arange(3,100, 3),'loss':['linear','square','exponential'],'learning_rate':[0.1,0.01,0.001],'n_estimators':[10,100,200]}),\n",
    "        \n",
    "        'Gradient Boosting' : (GradientBoostingRegressor(),{'n_neighbors': np.arange(3,100, 3),'loss':['ls','lad','huber','quantile'],'learning_rate':[0.1,0.01,0.001],'n_estimators':[10,100,200]})\n",
    "        }\n",
    "\n",
    "        \n",
    "#train and predict each model with evaluation metric as well making a for loop\n",
    "metric_values = {}\n",
    "for name , (model, params) in models.items():\n",
    "    #create a pipeline  with preprocessor\n",
    "    pipeline=Pipeline(steps=[('preprocessor', preprocessor),('model',model)])\n",
    "\n",
    "    #grid search cross validation\n",
    "    grid_search=GridSearchCV(pipeline,params,cv=5)\n",
    "\n",
    "    #fit pipeline\n",
    "    grid_search.fit(x_train,y_train)\n",
    "\n",
    "   \n",
    "    #make prediction on each model\n",
    "    y_pred=grid_search.predict(x_test)\n",
    "    \n",
    "      # Evaluate each model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Store metric values for each model\n",
    "    metric_values[name] = {'mse': mse, 'mae': mae, 'r2_score': r2}\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(name, \"mean squared error:\", mse)\n",
    "    print(name, \"mean absolute error:\", mae)\n",
    "    print(name, \"r2 score:\", r2)\n",
    "    print()\n",
    "\n",
    "# Find the best model based on each metric\n",
    "best_model_mse = min(metric_values, key=lambda x: metric_values[x]['mse'])\n",
    "best_model_mae = min(metric_values, key=lambda x: metric_values[x]['mae'])\n",
    "best_model_r2 = max(metric_values, key=lambda x: metric_values[x]['r2_score'])\n",
    "\n",
    "print(\"Best Model based on MSE:\", best_model_mse)\n",
    "print(\"Best Model based on MAE:\", best_model_mae)\n",
    "print(\"Best Model based on R2 Score:\", best_model_r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross validation on classifier problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Haseeb\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Haseeb\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Mean Accuracy: 0.9733333333333334\n",
      "\n",
      "Classifier: Decision Tree\n",
      "Mean Accuracy: 0.9533333333333335\n",
      "\n",
      "Classifier: Random Forest\n",
      "Mean Accuracy: 0.9600000000000002\n",
      "\n",
      "Classifier: SVM\n",
      "Mean Accuracy: 0.9666666666666668\n",
      "\n",
      "Classifier: kNN\n",
      "Mean Accuracy: 0.9733333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# # Define the classifiers\n",
    "# classifiers = {\n",
    "#     'Logistic Regression': LogisticRegression(),\n",
    "#     'Decision Tree': DecisionTreeClassifier(),\n",
    "#     'Random Forest': RandomForestClassifier(),\n",
    "#     'SVM': SVC(),\n",
    "#     'kNN': KNeighborsClassifier()\n",
    "# }\n",
    "\n",
    "# # Create the KFold object\n",
    "# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Iterate over the classifiers\n",
    "# for name, classifier in classifiers.items():\n",
    "#     # Perform cross-validation\n",
    "#     scores = cross_val_score(classifier, X, y, cv=kfold)\n",
    "#     accuracy = np.mean(scores)\n",
    "    \n",
    "#     # Print the results\n",
    "#     print(\"Classifier:\", name)\n",
    "#     print(\"Mean Accuracy:\", accuracy)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
